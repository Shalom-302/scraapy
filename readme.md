Absolument. Voici le `README.md` complet, correctement format√© en Markdown, pr√™t √† √™tre copi√©-coll√© dans votre projet.

---

# ü§ñ Agent de Veille "Conscience Tech Africaine"

Cet agent intelligent est un outil de veille strat√©gique con√ßu pour analyser l'actualit√© technologique mondiale et en extraire des le√ßons critiques, des probl√©matiques et des opportunit√©s sp√©cifiques au continent africain. Il va au-del√† du simple scraping en utilisant un LLM pour g√©n√©rer une analyse g√©ostrat√©gique, transformant l'information brute en intelligence actionnable.

## üéØ Mission

La mission de cet agent est de servir de **strat√®ge pour le d√©veloppement technologique africain**. Pour chaque actualit√© mondiale, il r√©pond √† la question fondamentale : **"Et alors ? Qu'est-ce que cela signifie pour nous, en Afrique ?"**. L'objectif est de catalyser un √©veil de conscience et d'identifier des opportunit√©s concr√®tes pour renforcer la souverainet√© num√©rique et l'innovation sur le continent.

## ‚ú® Fonctionnalit√©s Cl√©s

-   **Scraping Multi-Sites :** Collecte automatiquement les derniers articles des principales sources technologiques (Techmeme, TechCabal, etc.).
-   **Extraction de Contenu Robuste :** Utilise `trafilatura` pour nettoyer le HTML et n'extraire que le corps de l'article, ignorant les publicit√©s et les menus.
-   **Analyse en Deux Temps par LLM :**
    1.  **Contexte Global :** G√©n√®re un r√©sum√© neutre et identifie la probl√©matique universelle de l'article.
    2.  **Analyse Strat√©gique pour l'Afrique :** √âvalue l'impact local, r√©v√®le la probl√©matique sous-jacente, formule un "√©veil de conscience" et propose une piste d'opportunit√©.
-   **Scoring de Pertinence :** Attribue un score de 1 √† 10 pour quantifier l'importance strat√©gique de chaque article pour l'Afrique.
-   **Rapport Intelligent et Class√© :** G√©n√®re un rapport Markdown complet, tri√© par score de pertinence, pr√©sentant les articles les plus critiques en premier.
-   **Validation des Donn√©es :** Utilise **Pydantic** pour garantir que la sortie du LLM est toujours structur√©e et fiable, pr√©venant les erreurs d'analyse.
-   **Orchestration par LangGraph :** Le workflow complet est g√©r√© par un graphe d'√©tats robuste et modulaire.
-   **Interface Web Interactive :** Une application **Streamlit** simple permet de lancer la veille et de visualiser le rapport final.

## üöÄ D√©monstration

L'interface utilisateur est simple et directe. L'utilisateur lance la veille via la barre lat√©rale, et le rapport complet et class√© s'affiche dans la zone principale.

*[Ins√©rer une capture d'√©cran ou un GIF de l'application Streamlit ici]*

## üõ†Ô∏è Stack Technologique

-   **Orchestration :** LangGraph
-   **Mod√©lisation IA :** LangChain, DeepSeek API
-   **Interface Utilisateur :** Streamlit
-   **Scraping & Parsing :** Requests, BeautifulSoup, Trafilatura
-   **Validation de Donn√©es :** Pydantic
-   **Langage :** Python 3.9+

## üèóÔ∏è Architecture

L'agent est construit sur une architecture modulaire et √©v√©nementielle orchestr√©e par LangGraph.

### Sch√©ma de Flux G√©n√©ral

```mermaid
graph TD
    subgraph "Interface Utilisateur"
        A[üë§ Utilisateur] -->|Lance la Veille| B[üñ•Ô∏è Frontend Streamlit];
    end
    subgraph "Logique Applicative"
        B -->|Appelle run_veile_workflow()| C[‚öôÔ∏è Backend Runner];
        C -->|Invoque le graphe| D[üåê LangGraph Workflow];
    end
    D -->|Retourne le r√©sultat final| C;
    C -->|Formate les donn√©es| B;
    B -->|Affiche le Rapport| A;
```

### Sch√©ma D√©taill√© du Graphe LangGraph

```mermaid
graph LR
    subgraph "Workflow LangGraph"
        START((Start)) --> N1[üß† planner];
        N1 --> ROUTER{üîÄ should_continue};
        ROUTER -- "continue_scraping" --> N2[üß† scraper_dispatcher];
        N2 -- "utilise" --> REGISTRE[üìö Registre];
        N2 -- "scrape" --> SITES[üåç Sites Web];
        N2 --> N1;
        ROUTER -- "end_scraping" --> N3[üß† extract_analyze_and_report];
        N3 -- "analyse" --> LLM[üåç API DeepSeek];
        N3 -- "valide avec" --> SCHEMA[üóÉÔ∏è Pydantic Model];
        N3 --> FINISH((End));
    end
```

## ‚öôÔ∏è Installation et Lancement

Suivez ces √©tapes pour lancer l'agent sur votre machine locale.

### 1. Pr√©requis
-   Python 3.9 ou sup√©rieur
-   Git

### 2. Cloner le D√©p√¥t
```bash
git clone <url-du-depot>
cd <nom-du-depot>
```

### 3. Cr√©er un Environnement Virtuel
```bash
python -m venv .venv
# Sur Windows
.venv\Scripts\activate
# Sur macOS/Linux
source .venv/bin/activate
```

### 4. Installer les D√©pendances
```bash
pip install -r requirements.txt
```

### 5. Configurer les Variables d'Environnement
Cr√©ez un fichier `.env` √† la racine du projet en copiant le mod√®le `.env.example`.

**`.env.example`**
```ini
# Cl√© API pour le mod√®le de langage DeepSeek
DEEPSEEK_API_KEY="votre_cle_api_ici"

# Configuration optionnelle pour LangSmith (observabilit√©)
LANGSMITH_TRACING="true"
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY="votre_cle_langsmith_ici"
LANGSMITH_PROJECT="nom_de_votre_projet_langsmith"
```
Remplacez `"votre_cle_api_ici"` par votre v√©ritable cl√© API DeepSeek.

### 6. Lancer l'Application
```bash
streamlit run main.py
```
Ouvrez votre navigateur √† l'adresse `http://localhost:8501`.

## üìÇ Structure du Projet
```
.
‚îú‚îÄ‚îÄ backend_scraapy.py   # C≈ìur de la logique : graphe LangGraph, n≈ìuds, analyse LLM
‚îú‚îÄ‚îÄ main.py              # Fichier de l'application Streamlit (interface utilisateur)
‚îú‚îÄ‚îÄ requirements.txt     # Liste des d√©pendances Python
‚îú‚îÄ‚îÄ .env                 # Fichier pour les secrets (cl√©s API) - NE PAS COMMIT
‚îú‚îÄ‚îÄ .env.example         # Mod√®le pour le fichier .env
‚îî‚îÄ‚îÄ README.md            # Ce fichier
```

## üß© √âtendre l'Agent (Ajouter un Nouveau Site)

L'architecture est con√ßue pour √™tre facilement extensible. Pour ajouter un nouveau site √† la veille :

1.  **Cr√©ez une fonction de scraping d√©di√©e** dans `backend_scraapy.py`. Inspirez-vous de `scrape_techmeme` et utilisez le *travail de d√©tective* (inspecteur du navigateur) pour trouver le bon s√©lecteur CSS.

    ```python
    def scrape_nouveau_site(soup: BeautifulSoup, base_url: str) -> List[FoundArticle]:
        articles = []
        # Votre logique de scraping avec le bon s√©lecteur CSS
        selector = "h2.article-title a" 
        for link in soup.select(selector):
            # ... extraire titre et href ...
            articles.append({"title": title, "url": url, "source": "Nouveau Site"})
        return articles
    ```

2.  **Ajoutez la fonction au `SCRAPER_REGISTRY`** dans `backend_scraapy.py`.

    ```python
    SCRAPER_REGISTRY = {
        "https://www.techmeme.com/": scrape_techmeme,
        "https://techcabal.com/": scrape_techcabal,
        # ... autres sites ...
        "https://www.nouveausite.com/": scrape_nouveau_site, # Ajoutez la nouvelle ligne ici
    }
    ```
C'est tout ! L'agent prendra automatiquement en charge le nouveau site lors de sa prochaine ex√©cution.