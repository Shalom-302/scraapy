
**Analyse de votre structure de fichiers :**

-   `api.py` : Sugg√®re que vous avez une API REST (probablement FastAPI) pour exposer les r√©sultats ou lancer la veille par programme.
-   `frontend.py` / `main.py` : Ce sont vos points d'entr√©e pour l'interface Streamlit.
-   `scraap.py` : C'est votre backend, le c≈ìur logique que nous avons construit.
-   `scheduler.py` : Implique que vous avez mis en place une ex√©cution planifi√©e (par exemple, un cron job quotidien).

Le `README.md` doit refl√©ter cette structure professionnelle. Je vais r√©√©crire le `README.md` pour qu'il corresponde parfaitement √† votre projet, en expliquant le r√¥le de chaque fichier et comment lancer les diff√©rents composants.

---

### **Le `README.md` Final, Adapt√© √† Votre Projet**

Voici la version finale. Effacez le contenu de votre `readme.md` actuel et remplacez-le par celui-ci.

```markdown
# ü§ñ Agent de Veille "Conscience Tech Africaine"

Cet agent intelligent est un outil de veille strat√©gique con√ßu pour analyser l'actualit√© technologique mondiale et en extraire des le√ßons critiques, des probl√©matiques et des opportunit√©s sp√©cifiques au continent africain. Il va au-del√† du simple scraping en utilisant un LLM pour g√©n√©rer une analyse g√©ostrat√©gique, transformant l'information brute en intelligence actionnable.

## ‚ú® Fonctionnalit√©s Cl√©s

-   **Scraping Multi-Sites :** Collecte automatiquement les derniers articles des principales sources technologiques (Techmeme, TechCabal, etc.).
-   **Analyse Strat√©gique Compl√®te :** Pour chaque article, g√©n√®re un r√©sum√© neutre, identifie la probl√©matique globale, puis fournit une analyse contextuelle pour l'Afrique (impact, √©veil de conscience, opportunit√©s).
-   **Scoring de Pertinence :** Attribue un score de 1 √† 10 pour quantifier l'importance strat√©gique de chaque article pour l'Afrique, permettant un tri efficace.
-   **Orchestration Robuste :** Le workflow complet est g√©r√© par **LangGraph**, assurant une ex√©cution modulaire et r√©siliente.
-   **Validation des Donn√©es :** Utilise **Pydantic** pour garantir que la sortie du LLM est toujours structur√©e et fiable.
-   **Modes d'Ex√©cution Multiples :** Peut √™tre lanc√© via une interface web interactive (Streamlit), une API REST, ou de mani√®re planifi√©e.

## üèóÔ∏è Architecture

Le projet est con√ßu avec une s√©paration claire des responsabilit√©s, le rendant robuste et maintenable.

-   **Backend (`scraap.py`) :** Le c≈ìur du syst√®me. Contient le graphe LangGraph, les fonctions de scraping (`BeautifulSoup`), et la logique d'analyse LLM.
-   **Frontend (`frontend.py`) :** Une interface web interactive construite avec **Streamlit** pour des d√©monstrations et une utilisation manuelle.
-   **API (`api.py`) :** Une interface REST (probablement **FastAPI**) qui expose la logique de veille, permettant √† d'autres services de consommer les r√©sultats.
-   **Scheduler (`scheduler.py`) :** Un script pour lancer la veille de mani√®re automatis√©e et p√©riodique (ex: tous les jours √† 8h).

```mermaid
graph TD
    subgraph "D√©clencheurs"
        A[üë§ Utilisateur] --> UI[üñ•Ô∏è Interface Streamlit];
        B[üïí T√¢che Planifi√©e] --> SCHED[‚öôÔ∏è scheduler.py];
        C[üíª Autre Service] --> API[üîå api.py];
    end
    
    subgraph "C≈ìur Applicatif"
        UI --> BACKEND[üß† backend: scraap.py];
        SCHED --> BACKEND;
        API --> BACKEND;
        BACKEND -- Orchestre avec --> LG[üåê LangGraph];
    end

    subgraph "Services Externes"
        LG -- Scrape --> SITES[üåç Sites Web];
        LG -- Analyse --> LLM[ü§ñ API DeepSeek];
    end
```

## ‚öôÔ∏è Installation et Lancement

### 1. Pr√©requis
-   Python 3.9+
-   Git

### 2. Installation
```bash
# Cloner le d√©p√¥t
git clone <url-du-depot>
cd <nom-du-depot>

# Cr√©er et activer un environnement virtuel
python -m venv .venv
source .venv/bin/activate  # ou .venv\Scripts\activate sur Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### 3. Configuration
Cr√©ez un fichier `.env` √† la racine du projet et remplissez-le avec vos cl√©s API, en vous basant sur `.env.example` (s'il existe).

```ini
DEEPSEEK_API_KEY="votre_cle_api_ici"
# ... autres variables d'environnement si n√©cessaire ...
```

### 4. Modes d'Ex√©cution

Vous pouvez lancer l'agent de trois mani√®res diff√©rentes :

#### a) Mode Interactif (Interface Web)
Id√©al pour les d√©monstrations et l'utilisation manuelle.
```bash
# Assurez-vous que votre fichier frontend s'appelle `frontend.py` ou ajustez la commande
streamlit run frontend.py
```
Ouvrez votre navigateur √† l'adresse `http://localhost:8501`.

#### b) Mode API
Pour int√©grer l'agent √† d'autres applications.
```bash
# Lance le serveur API (en supposant l'utilisation de FastAPI/Uvicorn)
uvicorn api:app --reload
```
L'API sera accessible sur `http://localhost:8000`.

#### c) Mode Planifi√© (T√¢che de Fond)
Pour une veille automatis√©e et r√©guli√®re.
```bash
# Lance le script du scheduler
python scheduler.py
```

## üìÇ Structure du Projet

```
.
‚îú‚îÄ‚îÄ api.py               # Serveur API (FastAPI/Flask) pour exposer la logique.
‚îú‚îÄ‚îÄ frontend.py          # Application web interactive (Streamlit).
‚îú‚îÄ‚îÄ main.py              # Potentiel point d'entr√©e alternatif ou script de lancement.
‚îú‚îÄ‚îÄ scraap.py            # C≈ìur logique : LangGraph, scraping, analyse. (Suggestion: renommer en backend.py)
‚îú‚îÄ‚îÄ scheduler.py         # Script pour l'ex√©cution planifi√©e de la veille.
|
‚îú‚îÄ‚îÄ requirements.txt     # D√©pendances Python.
‚îú‚îÄ‚îÄ .env                 # Fichier des secrets (cl√©s API).
‚îî‚îÄ‚îÄ README.md            # Ce fichier.
```

## üß© √âtendre l'Agent (Ajouter un Nouveau Site)

L'architecture est con√ßue pour √™tre facilement extensible via le **Registre de Scrapers**.

1.  **Cr√©ez une fonction de scraping d√©di√©e** dans `scraap.py` en utilisant `BeautifulSoup` pour trouver le bon s√©lecteur CSS.

    ```python
    def scrape_nouveau_site(soup: BeautifulSoup, base_url: str) -> List[FoundArticle]:
        articles = []
        selector = "h2.article-title a" # Exemple de s√©lecteur
        for link in soup.select(selector):
            # ... extraire titre et href ...
            articles.append({"title": title, "url": url, "source": "Nouveau Site"})
        return articles
    ```

2.  **Ajoutez la fonction au `SCRAPER_REGISTRY`** dans `scraap.py`.

    ```python
    SCRAPER_REGISTRY = {
        # ... autres sites ...
        "https://www.nouveausite.com/": scrape_nouveau_site,
    }
    ```

